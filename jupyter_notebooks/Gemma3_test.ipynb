{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gemma 3  \n",
        "Gemma 3 is released on 3/12/2025.\n",
        "\n",
        "I'll test Gemma-3-4b-instruct in this notebook.  \n",
        "https://huggingface.co/google/gemma-3-4b-it"
      ],
      "metadata": {
        "id": "DxuEUiQs2Bhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3\n"
      ],
      "metadata": {
        "id": "_4Cybv-t2zdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pipeline api"
      ],
      "metadata": {
        "id": "PtzTk0b23BDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = userdata.get('HF_API_KEY') # replace with your own huggingface token"
      ],
      "metadata": {
        "id": "MyjgD5Xu3Rp0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"image-text-to-text\",\n",
        "    model=\"google/gemma-3-4b-it\",\n",
        "    device=\"cuda\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n"
      ],
      "metadata": {
        "id": "myUlLGZm3CoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/p-blog/candy.JPG\"},\n",
        "            {\"type\": \"text\", \"text\": \"What animal is on the candy?\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "output = pipe(text=messages, max_new_tokens=200)\n",
        "print(output[0][\"generated_text\"][-1][\"content\"])\n",
        "# Okay, let's take a look!\n",
        "# Based on the image, the animal on the candy is a **turtle**.\n",
        "# You can see the shell shape and the head and legs.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c8Z-Ra83EtM",
        "outputId": "331adc67-93a4-489e-890d-1c2cabe5aa6b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, let's take a look at the candy!\n",
            "\n",
            "Based on the image, the animal on the candy is a **turtle**. You can see the shell shape and the head and legs. \n",
            "\n",
            "Do you want to know anything more about these candies?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gradio interface|"
      ],
      "metadata": {
        "id": "UtWh6Vj35ra1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "xcrgT0KK8MKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.image_utils import load_image\n",
        "import gradio as gr\n",
        "def process_image_and_generate(user_message, image):\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": load_image(image)},\n",
        "                {\"type\": \"text\", \"text\": user_message}\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    output = pipe(text=messages, max_new_tokens=1000)\n",
        "    response = output[0][\"generated_text\"][-1][\"content\"]\n",
        "    return response\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Gemma-3-4b-it chat\")\n",
        "    image_input = gr.Image(type=\"pil\", label=\"Upload an image\")\n",
        "    user_message = gr.Textbox(label=\"User Prompt\")\n",
        "    with gr.Accordion('Answer:', open=True):\n",
        "        image_output = gr.Markdown(label=\"Output\",)\n",
        "    image_button = gr.Button(\"Process Image\")\n",
        "\n",
        "    image_button.click(\n",
        "        fn=process_image_and_generate,\n",
        "        inputs=[user_message, image_input],\n",
        "        outputs=image_output,\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "rQi2rGA75C4P",
        "outputId": "b1f9de21-8be9-46c5-c201-5329acdd0b19"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://971706fbd0f4b36c2d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://971706fbd0f4b36c2d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K6MorKL87NG-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}